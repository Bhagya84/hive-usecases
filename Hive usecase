Usecase#1:

1. Login to Mysql and execute the sql file to load the custpayments table:
source /home/hduser/hiveusecase/custpayments_ORIG.sql

Result:
Query OK, 0 rows affected (0.00 sec)

Query OK, 0 rows affected (0.00 sec)

Query OK, 1 row affected, 1 warning (0.00 sec)

Database changed
Query OK, 0 rows affected (0.05 sec)

Query OK, 0 rows affected (0.02 sec)

Query OK, 122 rows affected (0.03 sec)
Records: 122  Duplicates: 0  Warnings: 0

2. Write sqoop command to import data from customerpayments table with 2 mappers, with enclosed by " (As we have ',' in the data itself we are importing in sqoop using --enclosed-by option into the location /user/hduser/custpayments).

sqoop import --connect jdbc:mysql://localhost/custpayments --username root --password root -table customers -m 2 --split-by customernumber --target-dir /user/hduser/custpayments/ --delete-target-dir --enclosed-by '\"';

3. Create a hive external table and load the sqoop imported data to the hive table called custpayments. As we have ',' in the data itself we are using quotedchar option below with the csv serde option as given below as example, create the table with all columns.

create external table custpayments(customerNumber int,customerName string,contactLastName string,contactFirstName string,phone string,addressLine1 string,
addressLine2 string,city string,state string,postalCode int,country string,salesRepEmployeeNumber int,creditLimit double)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES (
"separatorChar" = ",",
"quoteChar" = "\"") 
LOCATION '/user/hduser/custpayments/';

4.Copy the payments.txt into hdfs location /user/hduser/paymentsdata/ and Create an external table namely payments with customernumber, checknumber, paymentdate, amount columns to point the imported payments data.

hadoop fs -mkdir -p /user/hduser/paymentsdata

hadoop fs -put /home/hduser/payments.txt /user/hduser/paymentsdata

create external table payments(customernumber int,checknumber string,paymentdate date,amount double)
row format delimited fields terminated by ','
stored as textfile 
location '/user/hduser/paymentsdata';

5. Create an external table called cust_payments in avro format and load data by doing inner join of custmaster and payments tables, using insert select customernumber, contactfirstname,contactlastname,phone, creditlimit from custmaster and paymentdate, amount columns from payments table

create external table cust_payments(custno int,contactfirstname string,contactlastname string,phone string,creditlimit double,paymentdate date,amount double)
row format delimited fields terminated by '~'
stored as avro 
location '/user/hduser/custpaymentsavro';

insert into table cust_payments select a.customerNumber,a.contactfirstname,a.contactlastname,a.phone,
a.creditlimit,b.paymentdate,b.amount from custdb.custpayments a JOIN custdb.payments b
ON a.customerNumber = b.customernumber; 

6. Create a view called custpayments_vw to only display customernumber,creditlimit,paymentdate and
amount selected from cust_payments.

create view custpayments_vw as select custno,creditlimit,paymentdate, amount from cust_payments;

7. Extract only customernumber,creditlimit,paymentdate and amount columns either using the above
view/cust_payments table into hdfs location /user/hduser/custpaymentsexport with '|' delimiter.

insert overwrite directory /user/hduser/custpaymentsexport 
row format delimited fields terminated by '|' 
select * from custpayments_vw;

8. Export the data from the /user/hduser/custpaymentsexport location to mysql table called
cust_payments using sqoop export with staging table option using records per statement 100 and
mappers 3.

a. Create mysql table cust_payments with 4 columns customernumber,creditlimit,paymentdate and amount
b. Create mysql table custpayments_stage with 4 columns customernumber,creditlimit,paymentdate and amount

sqoop export -Dsqoop.export.records.per.statement=100 --connect jdbc:mysql://localhost/custdb --username root --password root --table cust_payments -m 3 --export-dir custpaymentsexport --fields-terminated-by '|' --lines-terminated-by '\n' --batch --staging-table custpayments_stage --clear-staging-table --columns custno, credlim, paymentdate,amount;

Usecase#2:
1. Copy the below fixed data into a linux file, load into a hive table called cust_fixed_raw in a column
rawdata.

a. vi rawdata_cust

1 Lara        chennai   55 2016-09-2110000
2 vasudevan   banglore  43 2016-09-2390000
3 Paul        chennai   33 2019-02-2020000
4 David Hanna New Jersey29 2019-04-22

b. create table cust_fixed_raw(rawdata string);

c. Load data local inpath '/home/hduser/rawdata_cust' overwrite into table cust_fixed_raw;

2. Create a temporary table called cust_delimited_parsed_temp with columns such as
id,name,city,age,dt,amt and load the cust_fixed_raw table using substr.

for eg to select id column : select trim(substr(rawdata,1,3)) from cust_fixed_raw;

Solution:
create temporary table cust_delimited_parsed_temp(id int,name string,city string,age int,dt date,amt double);		-> /tmp/hive/hduser/ae6aa95f-af11-45f8-ba38-9e07993674a4 created in hdfs

insert into table cust_delimited_parsed_temp 
select trim(substr(rawdata,1,2)) as id,trim(substr(rawdata,3,11)) as name,
trim(substr(rawdata,14,10)) as city,trim(substr(rawdata,24,2)) as age,
trim(substr(rawdata,27,10)) as dt,trim(substr(rawdata,37,10))as amt from cust_fixed_raw;

hdfs:
hadoop fs -cat /tmp/hive/hduser/ae6aa95f-af11-45f8-ba38-9e07993674a4/_tmp_space.db/e066c60a-8695-468a-8095-8aab483afa29/000000_0
20/09/23 09:56:24 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
1Larachennai552016-09-2110000.0
2vasudevanbanglore432016-09-2390000.0
3Paulchennai332019-02-2020000.0
4David HannaNew Jersey292019-04-22\N

hive:
select * from cust_delimited_parsed_temp;
OK
1	Lara	chennai	55	2016-09-21	10000.0
2	vasudevan	banglore	43	2016-09-23	90000.0
3	Paul	chennai	33	2019-02-20	20000.0
4	David Hanna	New Jersey	29	2019-04-22	NULL
Time taken: 0.125 seconds, Fetched: 4 row(s)

insert into table cust_delimited_parsed_temp 
select trim(substr(rawdata,1,2)) as id,trim(substr(rawdata,3,11)) as name,trim(substr(rawdata,14,10)) 
as city,trim(substr(rawdata,24,2)) as age,trim(substr(rawdata,27,10)) as dt,trim(substr(rawdata,37,10))as amt from cust_fixed_raw;

3. Export only id, dt and amt column into a mysql table cust_fixed_mysql using sqoop export.

Solution:
create table cust_fixed_mysql(id int,dt date,amt double);	

insert overwrite directory '/user/hive/warehouse/custdb.db/cust_delimited_parsed_temp' 
row format delimited fields terminated by '|'
select id,dt,amt from cust_delimited_parsed_temp;

hdfs:
1|2016-09-21|10000.0
2|2016-09-23|90000.0
3|2019-02-20|20000.0
4|2019-04-22|\N

hive:
hive (custdb)> select id,dt,amt from cust_delimited_parsed_temp;
OK
id	dt	amt
1	2016-09-21	10000.0
2	2016-09-23	90000.0
3	2019-02-20	20000.0
4	2019-04-22	NULL

sqoop export --connect jdbc:mysql://localhost/custdb --username root --password root --table cust_fixed_mysql -m 1 --export-dir /user/hive/warehouse/custdb.db/cust_delimited_parsed_temp --fields-terminated-by '|' --lines-terminated-by '\n' --input-null-non-string '\\N';

mysql> select * from cust_fixed_mysql;
+------+------------+-------+
| id   | dt         | amt   |
+------+------------+-------+
|    1 | 2016-09-21 | 10000 |
|    2 | 2016-09-23 | 90000 |
|    3 | 2019-02-20 | 20000 |
|    4 | 2019-04-22 |  NULL |
+------+------------+-------+
4 rows in set (0.00 sec)

4. Load only chennai data to another table called cust_parsed_orc of type orc format partitioned based
on dt.

Solution:
create table cust_parsed_orc(id int,name string,city string,age int,amt double) partitioned by (dt date)
row format delimited fields terminated by ','
stored as orcfile; 	

/user/hive/warehouse/custdb.db/cust_parsed_orc is created in hdfs

insert overwrite table cust_parsed_orc partition(dt)
select trim(substr(rawdata,1,2)) as id,trim(substr(rawdata,3,11)) as name,trim(substr(rawdata,14,10)) 
as city,trim(substr(rawdata,24,2)) as age,trim(substr(rawdata,37,10))as amt,trim(substr(rawdata,27,10)) as dt from cust_fixed_raw where trim(substr(rawdata,14,10))='chennai';

hdfs:
Found 2 items
drwxr-xr-x   - hduser supergroup          0 2020-09-23 02:43 /user/hive/warehouse/custdb.db/cust_parsed_orc/dt=2016-09-21
drwxr-xr-x   - hduser supergroup          0 2020-09-23 02:43 /user/hive/warehouse/custdb.db/cust_parsed_orc/dt=2019-02-20

hive:
select * from cust_parsed_orc;
OK
cust_parsed_orc.id	cust_parsed_orc.name	cust_parsed_orc.city	cust_parsed_orc.age	cust_parsed_orc.amt	cust_parsed_orc.dt
1	Lara	chennai	55	10000.0	2016-09-21
3	Paul	chennai	33	20000.0	2019-02-20
Time taken: 0.136 seconds, Fetched: 2 row(s)

5. Create a json table called cust_parsed_json (to load into json use the following steps).

cd /home/hduser/hiveusecase

wget https://repo1.maven.org/maven2/org/apache/hive/hcatalog/hive-hcatalog-core/1.2.1/hive-
hcatalog-core-1.2.1.jar

add jar /home/hduser/hiveusecase/hive-hcatalog-core-1.2.1.jar;

add jar /usr/local/hive/lib/hivexmlserde-1.0.5.3.jar

create external table cust_parsed_json(id int, name string,city string, age int)
ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'
stored as textfile
location '/user/hduser/custjson';

6. Insert into the cust_parsed_json only non chennai data using insert select of id,name,city, age from
the cust_delimited_parsed_temp table.

Solution:
insert overwrite table cust_parsed_json
select id,name,city,age from cust_delimited_parsed_temp where city not in ('chennai');

hdfs:
{"id":2,"name":"vasudevan","city":"banglore","age":43}
{"id":4,"name":"David Hanna","city":"New Jersey","age":29}

hive:
select * from cust_parsed_json;
OK
2	vasudevan	banglore	43
4	David Hanna	New Jersey	29
Time taken: 0.11 seconds, Fetched: 2 row(s)

7. Schema migration:
Convert the XML table called xml_bank created in the actual usecase to JSON data by the same way like
step 5 using create table as select.

Solution:
create table xml_json
ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'
stored as textfile
location '/user/hduser/custxmljson'
as select * from retail.xml_bank;

hdfs:
{"customer_id":"0000-JTALA","income":200000,"demographics":{"spousedcat":"1","empcat":"2","gender":"F","jobsat":"1","homeown":"0","edcat":"1","hometype":"2","addresscat":"2","marital":"1","jobcat":"2","retire":"0","residecat":"4","agecat":"1"},"financial":{"income":"18","default":"0","creddebt":"1.003392","othdebt":"2.740608"}}
{"customer_id":"0000-KDELL","income":10000,"demographics":{"spousedcat":"1","empcat":"3","gender":"M","jobsat":"1","homeown":"0","edcat":"1","hometype":"3","addresscat":"2","marital":"1","jobcat":"2","retire":"1","residecat":"4","agecat":"2"},"financial":{"income":"20","default":"0","creddebt":"1.002292","othdebt":"2.113208"}}

hive:
select * from xml_json;
OK
0000-JTALA	200000	{"spousedcat":"1","empcat":"2","gender":"F","jobsat":"1","homeown":"0","edcat":"1","hometype":"2","addresscat":"2","marital":"1","jobcat":"2","retire":"0","residecat":"4","agecat":"1"}	{"income":"18","default":"0","creddebt":"1.003392","othdebt":"2.740608"}
0000-KDELL	10000	{"spousedcat":"1","empcat":"3","gender":"M","jobsat":"1","homeown":"0","edcat":"1","hometype":"3","addresscat":"2","marital":"1","jobcat":"2","retire":"1","residecat":"4","agecat":"2"}	{"income":"20","default":"0","creddebt":"1.002292","othdebt":"2.113208"}
Time taken: 0.081 seconds, Fetched: 2 row(s)

8. Import data from mysql directly creating static partition based on city=chennai as given below for
additional knowledge.

sqoop import \
--connect jdbc:mysql://localhost:3306/custdb \
--username root \
--password root \
--query "select custid,firstname,age from customer where city='chennai' and \$CONDITIONS" \
--target-dir /user/hduser/hiveext/ \
--split-by custid \
--hive-overwrite \
--hive-import \
--create-hive-table \
--hive-partition-key city \
--hive-partition-value 'chennai' \
--fields-terminated-by ',' \
--hive-table default.custinfo \
--direct

select * from default.custinfo;
OK
1	Arun	33	chennai
1	Arun	33	chennai
2	srini	33	chennai
1	Arun	33	chennai
1	Arun	33	chennai
2	srini	33	chennai
5	arun	23	chennai
5	arun	23	chennai
7	inceptez	3	chennai
7	inceptez	3	chennai
7	inceptez	3	chennai
Time taken: 0.201 seconds, Fetched: 11 row(s)

